{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849a8683",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Version 1: Baseline\n",
    "- Basic data augmentation (crop, flip, perspective, rotation, color jitter)\n",
    "- Established baseline performance\n",
    "\n",
    "### Version 2: Enhanced Augmentation\n",
    "- Stronger perspective warping (0.5 distortion)\n",
    "- More aggressive color jitter\n",
    "- Goal: Help distinguish similar characters (Geralt/Vesemir)\n",
    "- Result: Too aggressive, hurt training stability\n",
    "\n",
    "### Version 3: Balanced Approach\n",
    "- Sweet spot between V1 and V2\n",
    "- Moderate augmentation (0.45 perspective, reduced color jitter)\n",
    "- Removed random grayscale (hurt performance)\n",
    "- Added StepLR scheduler (drops LR at epochs 10, 20)\n",
    "- Result: Better convergence\n",
    "\n",
    "### Version 4: Dataset Improvement #1\n",
    "- **Key Change**: Enhanced 'Other' class with more white-haired characters\n",
    "- Same transforms as V3\n",
    "- Reason: Confusion matrix showed 'Other' frequently misclassified as Vesemir/Geralt\n",
    "- Result: Improved 'Other' class accuracy\n",
    "\n",
    "### Version 5: Adaptive Learning Rate\n",
    "- **Key Change**: Switched to ReduceLROnPlateau scheduler\n",
    "- Further dataset refinement\n",
    "- LR reduces when validation loss plateaus (patience=3)\n",
    "- Result: More intelligent fine-tuning\n",
    "\n",
    "### Version 6: Final Refinement\n",
    "- **Key Changes**: Additional dataset improvements + 30 epochs\n",
    "- Same architecture and scheduler as V5\n",
    "- **Final Performance**: ~85% validation accuracy\n",
    "- Best per-class results:\n",
    "  - Yennefer: 95%\n",
    "  - Ciri: 87%\n",
    "  - Triss: 86%\n",
    "  - Vesemir: 86%\n",
    "  - Other: 85%\n",
    "  - Geralt: 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b272e-ccfb-4b61-a515-785ee15a843a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splitfolders\n",
    "\n",
    "# Update this to your actual desktop path\n",
    "base_path = '/path/to/your/project'\n",
    "\n",
    "input_folder = os.path.join(base_path, \"dataset\")\n",
    "output_folder = os.path.join(base_path, \"split_data\")\n",
    "\n",
    "# Check if input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Found input folder at: {input_folder}\")\n",
    "    # Let's also see what classes we have\n",
    "    classes = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "    print(f\"Classes found: {classes}\")\n",
    "    \n",
    "    # Show image counts per class\n",
    "    for cls in classes:\n",
    "        count = len(os.listdir(os.path.join(input_folder, cls)))\n",
    "        print(f\"  {cls}: {count} images\")\n",
    "else:\n",
    "    print(\"Error: Could not find the input folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120cd40-f39b-4bd9-8020-a395b59ffee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with 80/20 ratio\n",
    "# seed=42 ensures reproducibility\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .2))\n",
    "\n",
    "print(\"--- Split Complete ---\")\n",
    "print(f\"Check: {output_folder} should now contain 'train' and 'val' folders.\")\n",
    "\n",
    "# Let's verify the split worked correctly\n",
    "print(\"\\n--- Verifying Split ---\")\n",
    "for split in ['train', 'val']:\n",
    "    split_path = os.path.join(output_folder, split)\n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"\\n{split.upper()} set:\")\n",
    "        classes = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
    "        for cls in classes:\n",
    "            count = len(os.listdir(os.path.join(split_path, cls)))\n",
    "            print(f\"  {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f1295-ba18-44a0-815d-3e9b2418d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomPerspective(distortion_scale=0.4, p=0.5),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Point to the split folders\n",
    "data_dir = os.path.join(base_path, \"split_data\")\n",
    "\n",
    "# Load datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# Create dataloaders (batch_size=16 like the binary model)\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=16, shuffle=True)\n",
    "               for x in ['train', 'val']}\n",
    "\n",
    "# Store class names and dataset sizes\n",
    "class_names = image_datasets['train'].classes\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "\n",
    "print(f\"Classes found: {class_names}\")\n",
    "print(f\"Training images: {dataset_sizes['train']}\")\n",
    "print(f\"Validation images: {dataset_sizes['val']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a0a52f-a03d-40af-aca5-998c94d77436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Get the actual counts per class in the training set\n",
    "train_counts = []\n",
    "for class_name in class_names:\n",
    "    class_folder = os.path.join(data_dir, 'train', class_name)\n",
    "    count = len(os.listdir(class_folder))\n",
    "    train_counts.append(count)\n",
    "    print(f\"{class_name}: {count} training images\")\n",
    "\n",
    "# Calculate weights inversely proportional to class frequency\n",
    "# Classes with fewer samples get higher weight\n",
    "train_counts = np.array(train_counts)\n",
    "class_weights = 1.0 / train_counts\n",
    "class_weights = class_weights / class_weights.sum() * len(class_names)  # Normalize\n",
    "\n",
    "# Convert to tensor\n",
    "class_weights = torch.FloatTensor(class_weights)\n",
    "\n",
    "print(\"\\n--- Class Weights ---\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}: {class_weights[i]:.4f}\")\n",
    "\n",
    "print(\"\\nThese weights will be applied to the loss function.\")\n",
    "print(\"Higher weight = model penalized more for getting that class wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v1-header",
   "metadata": {},
   "source": [
    "## Version 1: Baseline Model\n",
    "ResNet18 with all layers unfrozen, class weights, and basic augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9ae3fd-c6c7-4212-aff6-56d4b31268a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load pretrained ResNet18\n",
    "model = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# UNFREEZE all layers (like V4 of the binary model)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Replace final layer\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 6)  # 6 classes now!\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Final layer output: {model.fc.out_features} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c18b2a-c511-4580-92c6-b32389b54161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function WITH class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer: Low learning rate since we're fine-tuning the entire unfrozen network\n",
    "# Using 1e-5 like V4\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "print(\"Loss function: CrossEntropyLoss with class weights\")\n",
    "print(f\"Optimizer: Adam with lr=1e-5\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049d74d-86c0-4296-bb37-452ca158a268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=15):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Save the best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45458569-b3c6-48e6-b2bd-f2679d8fce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiclass_v1 = train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c724f1c-c0eb-43fb-9695-673e1bf9ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "models_dir = os.path.join(base_path, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(models_dir, \"witcher_multiclass_v1.pth\")\n",
    "torch.save(model_multiclass_v1.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ee516-767c-45a1-bf16-d82e33058327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_preds, val_labels = evaluate_model(model_multiclass_v1, dataloaders['val'], class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"\\nOverall Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfdb65c-54aa-43b2-ac44-d39970827385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Witcher Character Classifier', fontsize=14, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-CLASS ACCURACY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_correct:2d}/{class_total:2d} = {class_acc:.2%}\")\n",
    "\n",
    "# 3. DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Precision, Recall, F1)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2-header",
   "metadata": {},
   "source": [
    "## Version 2: Enhanced Augmentation\n",
    "Stronger perspective warping and aggressive color jitter to help distinguish similar characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a01e04-615f-4139-b760-239579c1aa32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2: ENHANCED TRANSFORMS\n",
    "# Goal: Help distinguish similar characters (like Geralt and Vesemir)\n",
    "\n",
    "data_transforms_v2 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        \n",
    "        # Stronger perspective warping\n",
    "        transforms.RandomPerspective(distortion_scale=0.5, p=0.6),\n",
    "        \n",
    "        # More aggressive color jittering to force focus on facial features\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        \n",
    "        # Random grayscale: forces model to learn structure, not just hair color\n",
    "        transforms.RandomGrayscale(p=0.15),\n",
    "        \n",
    "        # Rotation and blur (keep from V1)\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Reload datasets with new transforms\n",
    "image_datasets_v2 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_v2[x])\n",
    "                     for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_v2 = {x: DataLoader(image_datasets_v2[x], batch_size=16, shuffle=True)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(\"V2 Transforms loaded!\")\n",
    "print(\"Key changes:\")\n",
    "print(\"  - Stronger perspective warping (0.5 distortion)\")\n",
    "print(\"  - More color jittering (brightness/contrast/saturation 0.3)\")\n",
    "print(\"  - Random grayscale (15% chance)\")\n",
    "print(\"  - Increased rotation (20 degrees)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa16af0b-8cc3-4d52-9c93-7c6cb037018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2: Fresh model with learning rate scheduler\n",
    "model_v2 = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model_v2.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model_v2.fc.in_features\n",
    "model_v2.fc = nn.Linear(num_ftrs, 6)\n",
    "model_v2 = model_v2.to(device)\n",
    "\n",
    "# Loss with class weights (same as before)\n",
    "criterion_v2 = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer\n",
    "optimizer_v2 = optim.Adam(model_v2.parameters(), lr=0.00001)\n",
    "\n",
    "# LEARNING RATE SCHEDULER\n",
    "# Reduces LR by factor of 0.5 if val loss doesn't improve for 3 epochs\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_v2, \n",
    "    mode='min',           # Monitor validation loss (minimize it)\n",
    "    factor=0.5,           # Reduce LR by half\n",
    "    patience=3,           # Wait 3 epochs before reducing\n",
    "    verbose=True,         # Print when LR changes\n",
    "    min_lr=1e-7           # Don't go below this\n",
    ")\n",
    "\n",
    "print(\"Model V2 initialized!\")\n",
    "print(f\"Initial learning rate: {optimizer_v2.param_groups[0]['lr']}\")\n",
    "print(\"Scheduler: ReduceLROnPlateau (patience=3, factor=0.5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e179fd-4d12-436a-b18f-075e87c70b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v2(model, criterion, optimizer, scheduler, num_epochs=30):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    # Track metrics for analysis\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders_v2[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets_v2[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets_v2[phase])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # Store metrics\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            # Save best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # SCHEDULER STEP: Adjust learning rate based on validation loss\n",
    "        scheduler.step(history['val_loss'][-1])\n",
    "        \n",
    "        # Print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current LR: {current_lr:.2e}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function V2 ready!\")\n",
    "print(\"New features:\")\n",
    "print(\"  - Tracks training history (loss & acc)\")\n",
    "print(\"  - Uses learning rate scheduler\")\n",
    "print(\"  - Prints current LR each epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d81238-af4d-46db-abfc-cc2faf4b7bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train V2 for 30 epochs\n",
    "print(\"Starting V2 training with 30 epochs...\")\n",
    "print(\"Watch for LR reductions when validation stops improving!\\n\")\n",
    "\n",
    "model_v2_trained, history_v2 = train_model_v2(\n",
    "    model_v2, \n",
    "    criterion_v2, \n",
    "    optimizer_v2, \n",
    "    scheduler,\n",
    "    num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v3-header",
   "metadata": {},
   "source": [
    "## Version 3: Balanced Approach\n",
    "Sweet spot between V1 and V2 - moderate augmentation with StepLR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2419a-7eff-4abe-bc64-69347668b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: BALANCED TRANSFORMS\n",
    "# Sweet spot between V1 and V2\n",
    "\n",
    "data_transforms_v3 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        \n",
    "        # Stronger perspective (like V2, helps with angles)\n",
    "        transforms.RandomPerspective(distortion_scale=0.45, p=0.5),\n",
    "        \n",
    "        # Moderate color jittering (less than V2, more than V1)\n",
    "        # Helps distinguish hair colors without being too extreme\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2),\n",
    "        \n",
    "        # REMOVED: Random grayscale (it hurt more than helped)\n",
    "        \n",
    "        # Slightly more rotation than V1\n",
    "        transforms.RandomRotation(15),\n",
    "        \n",
    "        # Keep blur\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Reload datasets with V3 transforms\n",
    "image_datasets_v3 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_v3[x])\n",
    "                     for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_v3 = {x: DataLoader(image_datasets_v3[x], batch_size=16, shuffle=True)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(\"V3 Transforms loaded!\")\n",
    "print(\"\\nChanges from V2:\")\n",
    "print(\"  Kept: Stronger perspective (0.45 distortion)\")\n",
    "print(\"  Reduced: Color jitter (0.25 brightness/contrast, 0.2 saturation)\")\n",
    "print(\"  Removed: Random grayscale\")\n",
    "print(\"  Kept: Rotation at 15 degrees\")\n",
    "print(\"\\nV3 should learn faster than V2 but be more robust than V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eedf60-b62c-4b31-b9f3-723dfe482133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V3: Fresh model \n",
    "model_v3 = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model_v3.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model_v3.fc.in_features\n",
    "model_v3.fc = nn.Linear(num_ftrs, 6)\n",
    "model_v3 = model_v3.to(device)\n",
    "\n",
    "# Loss with class weights\n",
    "criterion_v3 = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer - same LR as V1 and V2\n",
    "optimizer_v3 = optim.Adam(model_v3.parameters(), lr=0.00001)\n",
    "\n",
    "# StepLR scheduler instead of ReduceLROnPlateau\n",
    "# Reduces LR every 10 epochs regardless of performance\n",
    "scheduler_v3 = optim.lr_scheduler.StepLR(\n",
    "    optimizer_v3,\n",
    "    step_size=10,    # Reduce every 10 epochs\n",
    "    gamma=0.5,       # Multiply LR by 0.5\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model V3 initialized!\")\n",
    "print(f\"Initial learning rate: {optimizer_v3.param_groups[0]['lr']}\")\n",
    "print(\"Scheduler: StepLR (reduces LR at epochs 10, 20)\")\n",
    "print(\"\\nThis ensures LR actually decreases during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a8d8c-4d50-48d3-9938-b2add978853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v3(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders_v3[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets_v3[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets_v3[phase])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Step the scheduler AFTER both train and val\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current LR: {current_lr:.2e}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function V3 ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a40ab89-2ac1-47c2-b7bd-b7c93ddea800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train V3 for 25 epochs\n",
    "print(\"Starting V3 training with 25 epochs...\")\n",
    "print(\"LR will drop at epochs 10 and 20\\n\")\n",
    "\n",
    "model_v3_trained, history_v3 = train_model_v3(\n",
    "    model_v3,\n",
    "    criterion_v3,\n",
    "    optimizer_v3,\n",
    "    scheduler_v3,\n",
    "    num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f10bf-e300-4d09-8c1e-efdeb935757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_preds, val_labels = evaluate_model(model_v3, dataloaders_v3['val'], class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"\\nOverall Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbccf692-618c-4098-8f4f-41010997ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Witcher Character Classifier', fontsize=14, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-CLASS ACCURACY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_correct:2d}/{class_total:2d} = {class_acc:.2%}\")\n",
    "\n",
    "# 3. DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Precision, Recall, F1)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v4-header",
   "metadata": {},
   "source": [
    "## Version 4: Updated Dataset\n",
    "Same as V3 but with additional images in 'other' class to reduce confusion with Geralt/Vesemir (This will require us to re-split the dataset again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54ffc8-565d-4ee8-942e-7db2c31ba3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splitfolders\n",
    "\n",
    "# Update this to your actual desktop path\n",
    "base_path = '/path/to/your/project'\n",
    "\n",
    "input_folder = os.path.join(base_path, \"dataset\")\n",
    "output_folder = os.path.join(base_path, \"split_data\")\n",
    "\n",
    "# Check if input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Found input folder at: {input_folder}\")\n",
    "    # Let's also see what classes we have\n",
    "    classes = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "    print(f\"Classes found: {classes}\")\n",
    "    \n",
    "    # Show image counts per class\n",
    "    for cls in classes:\n",
    "        count = len(os.listdir(os.path.join(input_folder, cls)))\n",
    "        print(f\"  {cls}: {count} images\")\n",
    "else:\n",
    "    print(\"Error: Could not find the input folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bf41f-beca-4182-9862-493809a5b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with 80/20 ratio\n",
    "# seed=42 ensures reproducibility\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .2))\n",
    "\n",
    "print(\"--- Split Complete ---\")\n",
    "print(f\"Check: {output_folder} should now contain 'train' and 'val' folders.\")\n",
    "\n",
    "# Let's verify the split worked correctly\n",
    "print(\"\\n--- Verifying Split ---\")\n",
    "for split in ['train', 'val']:\n",
    "    split_path = os.path.join(output_folder, split)\n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"\\n{split.upper()} set:\")\n",
    "        classes = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
    "        for cls in classes:\n",
    "            count = len(os.listdir(os.path.join(split_path, cls)))\n",
    "            print(f\"  {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde75f57-b3a4-4af8-8315-b4183a4d09f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4: BALANCED TRANSFORMS, SAME AS V3\n",
    "# Sweet spot between V1 and V2\n",
    "\n",
    "data_transforms_v4 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        \n",
    "        # Stronger perspective (like V2, helps with angles)\n",
    "        transforms.RandomPerspective(distortion_scale=0.45, p=0.5),\n",
    "        \n",
    "        # Moderate color jittering (less than V2, more than V1)\n",
    "        # Helps distinguish hair colors without being too extreme\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2),\n",
    "        \n",
    "        # REMOVED: Random grayscale (it hurt more than helped)\n",
    "        \n",
    "        # Slightly more rotation than V1\n",
    "        transforms.RandomRotation(15),\n",
    "        \n",
    "        # Keep blur\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Reload datasets with V4 transforms\n",
    "image_datasets_v4 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_v4[x])\n",
    "                     for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_v4 = {x: DataLoader(image_datasets_v4[x], batch_size=16, shuffle=True)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(\"V4 Transforms loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92c8a41-5470-4151-94fe-7452d9aa6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4: Fresh model \n",
    "model_v4 = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model_v4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model_v4.fc.in_features\n",
    "model_v4.fc = nn.Linear(num_ftrs, 6)\n",
    "model_v4 = model_v4.to(device)\n",
    "\n",
    "# Loss with class weights\n",
    "criterion_v4 = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer - same LR as V1 and V2\n",
    "optimizer_v4 = optim.Adam(model_v4.parameters(), lr=0.00001)\n",
    "\n",
    "# StepLR scheduler instead of ReduceLROnPlateau\n",
    "# Reduces LR every 10 epochs regardless of performance\n",
    "scheduler_v4 = optim.lr_scheduler.StepLR(\n",
    "    optimizer_v4,\n",
    "    step_size=10,    # Reduce every 10 epochs\n",
    "    gamma=0.5,       # Multiply LR by 0.5\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Model V4 initialized!\")\n",
    "print(f\"Initial learning rate: {optimizer_v4.param_groups[0]['lr']}\")\n",
    "print(\"Scheduler: StepLR (reduces LR at epochs 10, 20)\")\n",
    "print(\"\\nThis ensures LR actually decreases during training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c65090-5a3b-4189-8499-df15ba559cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v4(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders_v4[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets_v4[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets_v4[phase])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Step the scheduler AFTER both train and val\n",
    "        scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current LR: {current_lr:.2e}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function V4 ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567c2187-330d-4399-98bb-ca1f3e2966ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train V4 for 25 epochs\n",
    "print(\"Starting V4 training with 25 epochs...\")\n",
    "print(\"LR will drop at epochs 10 and 20\\n\")\n",
    "\n",
    "model_v4_trained, history_v4 = train_model_v4(\n",
    "    model_v4,\n",
    "    criterion_v4,\n",
    "    optimizer_v4,\n",
    "    scheduler_v4,\n",
    "    num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821f9a2-95ea-47f7-863b-02f57c01d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_preds, val_labels = evaluate_model(model_v4, dataloaders_v4['val'], class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"\\nOverall Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc8e93f-46eb-445d-9f18-7dbee019aa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Witcher Character Classifier', fontsize=14, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-CLASS ACCURACY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_correct:2d}/{class_total:2d} = {class_acc:.2%}\")\n",
    "\n",
    "# 3. DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Precision, Recall, F1)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v5-header",
   "metadata": {},
   "source": [
    "## Version 5: ReduceLROnPlateau\n",
    "Further dataset refinement (requiring a re-split) with adaptive learning rate that decreases when validation loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4d3fe-b145-467b-9c5f-03ea80334b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splitfolders\n",
    "\n",
    "# Update this to your actual desktop path\n",
    "base_path = '/path/to/your/project'\n",
    "\n",
    "input_folder = os.path.join(base_path, \"dataset\")\n",
    "output_folder = os.path.join(base_path, \"split_data\")\n",
    "\n",
    "# Check if input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Found input folder at: {input_folder}\")\n",
    "    # Let's also see what classes we have\n",
    "    classes = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "    print(f\"Classes found: {classes}\")\n",
    "    \n",
    "    # Show image counts per class\n",
    "    for cls in classes:\n",
    "        count = len(os.listdir(os.path.join(input_folder, cls)))\n",
    "        print(f\"  {cls}: {count} images\")\n",
    "else:\n",
    "    print(\"Error: Could not find the input folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7471e-d497-4800-af19-502807fb7000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with 80/20 ratio\n",
    "# seed=42 ensures reproducibility\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .2))\n",
    "\n",
    "print(\"--- Split Complete ---\")\n",
    "print(f\"Check: {output_folder} should now contain 'train' and 'val' folders.\")\n",
    "\n",
    "# Let's verify the split worked correctly\n",
    "print(\"\\n--- Verifying Split ---\")\n",
    "for split in ['train', 'val']:\n",
    "    split_path = os.path.join(output_folder, split)\n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"\\n{split.upper()} set:\")\n",
    "        classes = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
    "        for cls in classes:\n",
    "            count = len(os.listdir(os.path.join(split_path, cls)))\n",
    "            print(f\"  {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb6d85-a527-4b40-a881-6eee906f71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V5: BALANCED TRANSFORMS, SAME AS V3 and V4\n",
    "# Sweet spot between V1 and V2\n",
    "\n",
    "data_transforms_v5 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        \n",
    "        # Stronger perspective (like V2, helps with angles)\n",
    "        transforms.RandomPerspective(distortion_scale=0.45, p=0.5),\n",
    "        \n",
    "        # Moderate color jittering (less than V2, more than V1)\n",
    "        # Helps distinguish hair colors without being too extreme\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2),\n",
    "        \n",
    "        # Slightly more rotation than V1\n",
    "        transforms.RandomRotation(15),\n",
    "        \n",
    "        # Keep blur\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Reload datasets with V5 transforms\n",
    "image_datasets_v5 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_v5[x])\n",
    "                     for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_v5 = {x: DataLoader(image_datasets_v5[x], batch_size=16, shuffle=True)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(\"V5 Transforms loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be478cc-2576-41cb-971c-5d91520066b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V5: Fresh model \n",
    "model_v5 = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model_v5.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model_v5.fc.in_features\n",
    "model_v5.fc = nn.Linear(num_ftrs, 6)\n",
    "model_v5 = model_v5.to(device)\n",
    "\n",
    "# Loss with updated V5 class weights\n",
    "criterion_v5 = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer\n",
    "optimizer_v5 = optim.Adam(model_v5.parameters(), lr=0.00001)\n",
    "\n",
    "# Using ReduceLROnPlateau for smarter fine-tuning\n",
    "scheduler_v5 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_v5, \n",
    "    mode='min',    # Monitor when validation loss stops decreasing\n",
    "    factor=0.5,    # Reduce LR by half \n",
    "    patience=3     # Wait 3 epochs of no improvement before dropping\n",
    ")\n",
    "\n",
    "print(\"Model V5 initialized!\")\n",
    "print(f\"Initial learning rate: {optimizer_v5.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f8b286-ef69-4b62-a827-2d68b044bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v5(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Use dataloaders_v5\n",
    "            for inputs, labels in dataloaders_v5[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(image_datasets_v5[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets_v5[phase])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                \n",
    "                # Step the ReduceLROnPlateau scheduler based on val_loss\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            # Deep copy the model if it's the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current LR: {current_lr:.2e}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function V5 (with Plateau Scheduler) ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35f841-ab70-46e5-af7f-83751d5c5617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start V5 training\n",
    "print(\"Starting V5 training with 25 epochs...\")\n",
    "print(\"LR will drop if validation loss plateaus for 3 epochs\\n\")\n",
    "\n",
    "model_v5_trained, history_v5 = train_model_v5(\n",
    "    model_v5,\n",
    "    criterion_v5,\n",
    "    optimizer_v5,\n",
    "    scheduler_v5,\n",
    "    num_epochs=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93bebf-e178-4b94-ab1a-b2952c0f0a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_preds, val_labels = evaluate_model(model_v5, dataloaders_v5['val'], class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"\\nOverall Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d1d81-93cb-4c0f-b370-2f252f97d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Witcher Character Classifier', fontsize=14, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-CLASS ACCURACY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_correct:2d}/{class_total:2d} = {class_acc:.2%}\")\n",
    "\n",
    "# 3. DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Precision, Recall, F1)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954f95ae",
   "metadata": {},
   "source": [
    "### This is a function to test your model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646a3e3-b015-476e-a3d9-349d8d1e8f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def predict_image(model, image_path, class_names):\n",
    "    # Load and transform the image\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_t = data_transforms_v5['val'](img).unsqueeze(0).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        conf, preds = torch.max(probs, 1)\n",
    "        \n",
    "    print(f\"Prediction: {class_names[preds[0]]}\")\n",
    "    print(f\"Confidence: {conf.item()*100:.2f}%\")\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage (update path to your test image):\n",
    "# predict_image(model_v5_trained, 'path/to/your/test/image.jpg', image_datasets_v5['train'].classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1495b-b112-4c17-a6ff-ee8f2a0897bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "models_dir = os.path.join(base_path, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(models_dir, \"witcher_multiclass_v5.pth\")\n",
    "torch.save(model_v5.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v6-header",
   "metadata": {},
   "source": [
    "## Version 6: Final Refinement\n",
    "Additional dataset improvements and 30 training epochs for optimal performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825525dd-5aad-491e-bc66-5134daf95268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splitfolders\n",
    "\n",
    "# Update this to your actual desktop path\n",
    "base_path = '/path/to/your/project'\n",
    "\n",
    "input_folder = os.path.join(base_path, \"dataset\")\n",
    "output_folder = os.path.join(base_path, \"split_data\")\n",
    "\n",
    "# Check if input folder exists\n",
    "if os.path.exists(input_folder):\n",
    "    print(f\"Found input folder at: {input_folder}\")\n",
    "    # Let's also see what classes we have\n",
    "    classes = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "    print(f\"Classes found: {classes}\")\n",
    "    \n",
    "    # Show image counts per class\n",
    "    for cls in classes:\n",
    "        count = len(os.listdir(os.path.join(input_folder, cls)))\n",
    "        print(f\"  {cls}: {count} images\")\n",
    "else:\n",
    "    print(\"Error: Could not find the input folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875c411-7e4a-4537-a544-cc3cc44b444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split with 80/20 ratio\n",
    "# seed=42 ensures reproducibility\n",
    "splitfolders.ratio(input_folder, output=output_folder, seed=42, ratio=(.8, .2))\n",
    "\n",
    "print(\"--- Split Complete ---\")\n",
    "print(f\"Check: {output_folder} should now contain 'train' and 'val' folders.\")\n",
    "\n",
    "# Let's verify the split worked correctly\n",
    "print(\"\\n--- Verifying Split ---\")\n",
    "for split in ['train', 'val']:\n",
    "    split_path = os.path.join(output_folder, split)\n",
    "    if os.path.exists(split_path):\n",
    "        print(f\"\\n{split.upper()} set:\")\n",
    "        classes = [d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))]\n",
    "        for cls in classes:\n",
    "            count = len(os.listdir(os.path.join(split_path, cls)))\n",
    "            print(f\"  {cls}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d1acbf-4d2b-45a3-9d58-285cb916a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6: BALANCED TRANSFORMS, SAME AS V3 and V4\n",
    "# Sweet spot between V1 and V2\n",
    "\n",
    "data_transforms_v6 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.08, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        \n",
    "        # Stronger perspective (like V2, helps with angles)\n",
    "        transforms.RandomPerspective(distortion_scale=0.45, p=0.5),\n",
    "        \n",
    "        # Moderate color jittering (less than V2, more than V1)\n",
    "        # Helps distinguish hair colors without being too extreme\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.2),\n",
    "        \n",
    "        # Slightly more rotation than V1\n",
    "        transforms.RandomRotation(15),\n",
    "        \n",
    "        # Keep blur\n",
    "        transforms.RandomApply([transforms.GaussianBlur(kernel_size=5)], p=0.2),\n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Reload datasets with V6 transforms\n",
    "image_datasets_v6 = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms_v6[x])\n",
    "                     for x in ['train', 'val']}\n",
    "\n",
    "dataloaders_v6 = {x: DataLoader(image_datasets_v6[x], batch_size=16, shuffle=True)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "print(\"V6 Transforms loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba7bd9f-aaae-40d5-9b3a-a6012a07e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6: Fresh model \n",
    "model_v6 = models.resnet18(weights=\"DEFAULT\")\n",
    "\n",
    "# Unfreeze all layers\n",
    "for param in model_v6.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "num_ftrs = model_v6.fc.in_features\n",
    "model_v6.fc = nn.Linear(num_ftrs, 6)\n",
    "model_v6 = model_v6.to(device)\n",
    "\n",
    "# Loss with updated V6 class weights\n",
    "criterion_v6 = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer\n",
    "optimizer_v6 = optim.Adam(model_v6.parameters(), lr=0.00001)\n",
    "\n",
    "# NEW: Using ReduceLROnPlateau for smarter fine-tuning\n",
    "scheduler_v6 = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer_v6, \n",
    "    mode='min',    # Monitor when validation loss stops decreasing\n",
    "    factor=0.5,    # Reduce LR by half \n",
    "    patience=3     # Wait 3 epochs of no improvement before dropping\n",
    ")\n",
    "\n",
    "print(\"Model V6 initialized!\")\n",
    "print(f\"Initial learning rate: {optimizer_v6.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99738bde-1aef-4d5a-943f-d92ec4301d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v6(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders_v6[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / len(image_datasets_v6[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets_v6[phase])\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                \n",
    "                # Step the ReduceLROnPlateau scheduler based on val_loss\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            # Deep copy the model if it's the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        # Print current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Current LR: {current_lr:.2e}')\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "print(\"Training function V6 (with Plateau Scheduler) ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e695aa7-3a2a-48c3-ade7-dcfb0f659861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start V6 training\n",
    "print(\"Starting V6 training with 30 epochs...\")\n",
    "print(\"LR will drop if validation loss plateaus for 3 epochs\\n\")\n",
    "\n",
    "model_v6_trained, history_v6 = train_model_v6(\n",
    "    model_v6,\n",
    "    criterion_v6,\n",
    "    optimizer_v6,\n",
    "    scheduler_v6,\n",
    "    num_epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679e299-f5f7-40ce-85ad-3d04f8b2fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions and true labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return all_preds, all_labels\n",
    "\n",
    "# Run evaluation on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_preds, val_labels = evaluate_model(model_v6, dataloaders_v6['val'], class_names)\n",
    "\n",
    "# Calculate overall accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "val_accuracy = accuracy_score(val_labels, val_preds)\n",
    "print(f\"\\nOverall Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a86242-d0e2-489e-8c7c-8988ab76fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CONFUSION MATRIX\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_preds)\n",
    "\n",
    "# Plot confusion matrix as heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Witcher Character Classifier', fontsize=14, pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. PER-CLASS ACCURACY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS ACCURACY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_correct = cm[i, i]\n",
    "    class_total = cm[i, :].sum()\n",
    "    class_acc = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name:12s}: {class_correct:2d}/{class_total:2d} = {class_acc:.2%}\")\n",
    "\n",
    "# 3. DETAILED CLASSIFICATION REPORT\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Precision, Recall, F1)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c37e751-3ecf-43fe-bf6f-11429422ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "models_dir = os.path.join(base_path, \"models\")\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(models_dir, \"witcher_multiclass_v6.pth\")\n",
    "torch.save(model_v6.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
